{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Riemannian_LineSearch_Armijo",
      "provenance": [],
      "authorship_tag": "ABX9TyNQt7Zxy+FwseGQmAyu4LCS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhilash1910/AI-Geometric-Learning/blob/master/Chapter_2_Understanding_the_data/Riemannian_LineSearch_Armijo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e1To4wkk47m"
      },
      "source": [
        "## Riemmannian Line Search\n",
        "\n",
        "<img src=\"https://i.imgur.com/5O09OAq.png\">\n",
        "\n",
        "The Line Search is an optimzation algorithm which tries to minimize an objective function $f(x+\\alpha p)$ with respect to $f(x)$ by optimizing the step size. For optimizations in the line search , there are 2 methods:\n",
        "\n",
        "- Backtracking method starts with a large estimate of $\\alpha$  and iteratively shrinks it.\n",
        "- Adaptive method applies a variational $\\alpha$ by Armijo condition which tries to converge faster to the minima\n",
        "\n",
        "The armijo can be stated as :\n",
        "Based on a selected control parameter $c \\in (0,1)$, the Armijoâ€“Goldstein condition tests whether a step-wise movement from a current position $ x$  to a modified position $x+\\alpha p$ achieves an adequately corresponding decrease in the objective function. The condition is fulfilled, if \n",
        "$$f(x+\\alpha p) \\leq f(x) +\\alpha c m$$\n",
        "\n",
        "In this case, we take as inputs:\n",
        "\n",
        "The Riemannian manifold $M$ with a retraction policy $R$ the projector operator $\\pi_{x_{k}}: R^{n}= \\tau_{x_{k}}M$ , a real valued differentiable curve(potential function) $f$ with initial iterate $x_{0} \\in M$ , the Armijo line search scalar $c$ \n",
        "\n",
        "Operation:\n",
        "\n",
        "while $x_{k}$ does not sufficiently minimize $f$ we have to:\n",
        "- Take the euclidean gradient to the Riemann direction \n",
        "- $\\eta_{k} = - grad(f(x_{k})) = \\pi_{x_{k}}(- \\nabla f(x_{k}))$\n",
        "- We select $x_{k+1}$ as in Armijo step size $(\\tau_{k})$:\n",
        "   $$f(x_{k})- f(x_{k+1}) \\geq c(f(x_{k})- f(R_{x}(\\tau_{k} \\eta_{k})))$$\n",
        "- Increment $k= k+1$\n",
        "\n",
        "In this case, we will be implementing the Armijo condition on Line Search with pytorch for a simpler spherical manifold.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoGCDk_tiYyo",
        "outputId": "9b45e686-fa21-4eb4-b368-5bd279311671"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class LineSearchBackTracking:\n",
        "    \"\"\"\n",
        "    Back-tracking line-search based on linesearch\n",
        "    \"\"\"\n",
        "    def __init__(self, contraction_factor=.5, optimism=2,\n",
        "                 suff_decr=1e-4, maxiter=25, initial_stepsize=1):\n",
        "        self.contraction_factor = contraction_factor\n",
        "        self.optimism = optimism\n",
        "        self.suff_decr = suff_decr\n",
        "        self.maxiter = maxiter\n",
        "        self.initial_stepsize = initial_stepsize\n",
        "\n",
        "        self._oldf0 = None\n",
        "    def retr(self, X, U):\n",
        "        Y = X + U\n",
        "        return Y/torch.norm(Y)\n",
        "\n",
        "    def objective(self,points,y):\n",
        "        accumulator = 0\n",
        "        def dist_(U, V):\n",
        "          # Make sure inner product is between -1 and 1\n",
        "          U=torch.tensor(U)\n",
        "          V=torch.tensor(V)\n",
        "          inner_pdt=float(torch.dot(U, V))\n",
        "          inner = max(min(inner_pdt, 1), -1)\n",
        "          return np.arccos(inner)\n",
        "        \n",
        "        for i in range(len(points)):\n",
        "            accumulator += dist_(y, points[i]) ** 2\n",
        "        return accumulator / 2\n",
        "\n",
        "    def search(self, points, x, d, f0, df0):\n",
        "        \"\"\"\n",
        "        Function to perform backtracking line-search.\n",
        "        Arguments:\n",
        "            - objective\n",
        "                objective function to optimise\n",
        "            - manifold\n",
        "                manifold to optimise over\n",
        "            - x\n",
        "                starting point on the manifold\n",
        "            - d\n",
        "                tangent vector at x (descent direction)\n",
        "            - df0\n",
        "                directional derivative at x along d\n",
        "        \"\"\"\n",
        "        # Compute the norm of the search direction\n",
        "        norm_d = torch.norm(x, d)\n",
        "\n",
        "        if self._oldf0 is not None:\n",
        "            # Pick initial step size based on where we were last time.\n",
        "            alpha = 2 * (f0 - self._oldf0) / df0\n",
        "            # Look a little further\n",
        "            alpha *= self.optimism\n",
        "        else:\n",
        "            alpha = self.initial_stepsize / norm_d\n",
        "        alpha = float(alpha)\n",
        "\n",
        "        # Make the chosen step and compute the cost there.\n",
        "        newx = self.retr(x, alpha * d)\n",
        "        newf = self.objective(points,newx)\n",
        "        step_count = 1\n",
        "\n",
        "        # Backtrack while the Armijo criterion is not satisfied\n",
        "        while (newf > f0 + self.suff_decr * alpha * df0 and step_count <= self.maxiter):\n",
        "            # Reduce the step size\n",
        "            alpha = self.contraction_factor * alpha\n",
        "\n",
        "            # and look closer down the line\n",
        "            newx = self.retr(x, alpha * d)\n",
        "            print(\"next iterate suggested by the line-search: \",newx)\n",
        "            newf = self.objective(points,newx)\n",
        "            step_count = step_count + 1\n",
        "\n",
        "        # If we got here without obtaining a decrease, we reject the step.\n",
        "        if newf > f0:\n",
        "            alpha = 0\n",
        "            newx = x\n",
        "\n",
        "        stepsize = alpha * norm_d\n",
        "\n",
        "        self._oldf0 = f0\n",
        "        print(\"Converging at step: \", newx)\n",
        "        return stepsize, newx\n",
        "\n",
        "if __name__=='__main__':\n",
        "  print(\"Implementation of Riemannian Line Search Optimization by Backtracking on sphere manifold\")\n",
        "  ls=LineSearchBackTracking()\n",
        "  print(\"Input coordinates\")\n",
        "  x=torch.tensor([-0.65726771, -0.02678122,  0.7531812])\n",
        "  print(\"Parameters for direction of gradient and tangent\")\n",
        "  f0=1\n",
        "  d=-0.62339641\n",
        "  df0=0\n",
        "  points=[[-0.58831187, -0.02677797,  0.80819062],\n",
        "  [-0.55208236, -0.02669815,  0.83336203],\n",
        "  [-0.51477841, -0.02656636,  0.85691156],\n",
        "  [-0.47647259, -0.02638288,  0.87879338],\n",
        "  [-0.43723948, -0.02614804,  0.89896492],]\n",
        "  ls.search(points, x, d, f0, df0)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Implementation of Riemannian Line Search Optimization by Backtracking on sphere manifold\n",
            "Input coordinates\n",
            "Parameters for direction of gradient and tangent\n",
            "next iterate suggested by the line-search:  tensor([-0.6003, -0.5787, -0.5520])\n",
            "next iterate suggested by the line-search:  tensor([-0.6225, -0.5794, -0.5260])\n",
            "next iterate suggested by the line-search:  tensor([-0.6647, -0.5788, -0.4725])\n",
            "next iterate suggested by the line-search:  tensor([-0.7379, -0.5698, -0.3618])\n",
            "next iterate suggested by the line-search:  tensor([-0.8362, -0.5283, -0.1474])\n",
            "next iterate suggested by the line-search:  tensor([-0.8926, -0.4176,  0.1700])\n",
            "next iterate suggested by the line-search:  tensor([-0.8507, -0.2684,  0.4520])\n",
            "Converging at step:  tensor([-0.8507, -0.2684,  0.4520])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyGdtJtTvfxW",
        "outputId": "152f53bc-df14-42c0-dacb-817f9c6ec6a2"
      },
      "source": [
        "class LineSearchAdaptive:\n",
        "    '''\n",
        "    Adaptive line-search\n",
        "    '''\n",
        "\n",
        "    def __init__(self, contraction_factor=.5, suff_decr=.5, maxiter=10,\n",
        "                 initial_stepsize=1):\n",
        "        self._contraction_factor = contraction_factor\n",
        "        self._suff_decr = suff_decr\n",
        "        self._maxiter = maxiter\n",
        "        self._initial_stepsize = initial_stepsize\n",
        "        self._oldalpha = None\n",
        "\n",
        "    def retr(self, X, U):\n",
        "        Y = X + U\n",
        "        return Y/torch.norm(Y)\n",
        "\n",
        "    def objective(self,points,y):\n",
        "        accumulator = 0\n",
        "        def dist_(U, V):\n",
        "          # Make sure inner product is between -1 and 1\n",
        "          U=torch.tensor(U)\n",
        "          V=torch.tensor(V)\n",
        "          inner_pdt=float(torch.dot(U, V))\n",
        "          inner = max(min(inner_pdt, 1), -1)\n",
        "          return np.arccos(inner)\n",
        "\n",
        "    def search(self,points, x, d, f0, df0):\n",
        "        norm_d = man.norm(x, d)\n",
        "\n",
        "        if self._oldalpha is not None:\n",
        "            alpha = self._oldalpha\n",
        "        else:\n",
        "            alpha = self._initial_stepsize / norm_d\n",
        "        alpha = float(alpha)\n",
        "\n",
        "        newx = self.retr(x, alpha * d)\n",
        "        newf = self.objective(points,newx)\n",
        "        cost_evaluations = 1\n",
        "\n",
        "        while (newf > f0 + self._suff_decr * alpha * df0 and\n",
        "               cost_evaluations <= self._maxiter):\n",
        "            # Reduce the step size.\n",
        "            alpha *= self._contraction_factor\n",
        "\n",
        "            # Look closer down the line.\n",
        "            newx = self.retr(x, alpha * d)\n",
        "            newf = self.objective(points,newx)\n",
        "\n",
        "            cost_evaluations += 1\n",
        "\n",
        "        if newf > f0:\n",
        "            alpha = 0\n",
        "            newx = x\n",
        "\n",
        "        stepsize = alpha * norm_d\n",
        "\n",
        "        # Store a suggestion for what the next initial step size trial should\n",
        "        # be. On average we intend to do only one extra cost evaluation. Notice\n",
        "        # how the suggestion is not about stepsize but about alpha. This is the\n",
        "        # reason why this line search is not invariant under rescaling of the\n",
        "        # search direction d.\n",
        "\n",
        "        # If things go reasonably well, try to keep pace.\n",
        "        if cost_evaluations == 2:\n",
        "            self._oldalpha = alpha\n",
        "        # If things went very well or we backtracked a lot (meaning the step\n",
        "        # size is probably quite small), speed up.\n",
        "        else:\n",
        "            self._oldalpha = 2 * alpha\n",
        "\n",
        "        return stepsize, newx\n",
        "\n",
        "if __name__=='__main__':\n",
        "  print(\"Implementation of Adaptive Riemannian Line Search Optimization  on sphere manifold\")\n",
        "  ls=LineSearchBackTracking()\n",
        "  print(\"Input coordinates\")\n",
        "  x=torch.tensor([-0.65726771, -0.02678122,  0.7531812])\n",
        "  print(\"Parameters for direction of gradient and tangent\")\n",
        "  f0=1\n",
        "  d=-0.62339641\n",
        "  df0=0\n",
        "  points=[[-0.58831187, -0.02677797,  0.80819062],\n",
        "  [-0.55208236, -0.02669815,  0.83336203],\n",
        "  [-0.51477841, -0.02656636,  0.85691156],\n",
        "  [-0.47647259, -0.02638288,  0.87879338],\n",
        "  [-0.43723948, -0.02614804,  0.89896492],]\n",
        "  ls.search(points, x, d, f0, df0)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Implementation of Adaptive Riemannian Line Search Optimization  on sphere manifold\n",
            "Input coordinates\n",
            "Parameters for direction of gradient and tangent\n",
            "next iterate suggested by the line-search:  tensor([-0.6003, -0.5787, -0.5520])\n",
            "next iterate suggested by the line-search:  tensor([-0.6225, -0.5794, -0.5260])\n",
            "next iterate suggested by the line-search:  tensor([-0.6647, -0.5788, -0.4725])\n",
            "next iterate suggested by the line-search:  tensor([-0.7379, -0.5698, -0.3618])\n",
            "next iterate suggested by the line-search:  tensor([-0.8362, -0.5283, -0.1474])\n",
            "next iterate suggested by the line-search:  tensor([-0.8926, -0.4176,  0.1700])\n",
            "next iterate suggested by the line-search:  tensor([-0.8507, -0.2684,  0.4520])\n",
            "Converging at step:  tensor([-0.8507, -0.2684,  0.4520])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        }
      ]
    }
  ]
}